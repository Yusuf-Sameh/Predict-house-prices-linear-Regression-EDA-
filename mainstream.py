import streamlit as st
import pandas as pd

##################################################################################################################################

hide_streamlit_style = """
            <style>
            #MainMenu {visibility: hidden;}
            footer {visibility: hidden;}
            </style>
            """
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

##################################################################################################################################

st.sidebar.title('Choose a Part of the Project')
option = st.sidebar.selectbox(
    'Choose a part',
    ('About', '* Test the Model', '* Full Notebook with Output', '1- Basic Libraries & Data' ,'2- EDA', '3- Data Preprocessing', '4- Preparing Data & Modeling'))

##################################################################################################################################

df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')

##################################################################################################################################

if option == 'About':
    st.write('This Data App & Project \n\nBy: Youssef Sameh\n\nIn: July 2023\n\ncontact: youssefpasha49@gmail.com')
    st.divider()

##################################################################################################################################
if option == '* Test the Model':
    st.header('Test The Model')
    st.write('In this section you can test the model and it is based on entering your inputs into the regression equation generated by the linear regression model')
    st.write('Not all independent variables are shown because they are many, and only some of them are shown to facilitate use')
    st.divider()
    st.write('Scroll down to see your results after you entering your inputs')
    st.divider()
    st.markdown("""
                ## Data Description

                - **LotFrontage:** Linear feet of street connected to property.
                - **LotArea:** Lot size in square feet.
                - **OverallQual:** Overall material and finish quality.
                - **YearBuilt:** Original construction date.
                - **YearRemodAdd:** Remodel date.
                - **MasVnrArea:** Masonry veneer area in square feet.
                - **BsmtFinSF1:** Type 1 finished square feet.
                - **1stFlrSF:** First Floor square feet.
                - **2ndFlrSF:** Second floor square feet.
                - **GrLivArea:** Above grade (ground) living area square feet.
                - **FullBath:** Full bathrooms above grade.
                - **HalfBath:** Half baths above grade.
                - **TotRmsAbvGrd:** Total rooms above grade (does not include bathrooms).
                - **Fireplaces:** Number of fireplaces.
                - **GarageYrBlt:** Year garage was built.
                - **GarageCars:** Size of garage in car capacity.
                - **GarageArea:** Size of garage in square feet.
                - **WoodDeckSF:** Wood deck area in square feet.
                - **OpenPorchSF:** Open porch area in square feet.
                - **MSZoning:** The general zoning classification.
                - A: Agriculture.
                - C: Commercial.
                - FV: Floating Village Residential.
                - I: Industrial.
                - RH: Residential High Density.
                - RL: Residential Low Density.
                - RP: Residential Low Density Park.
                - RM: Residential Medium Density.
                - **LotShape:** General shape of property.
                - Reg: Regular.
                - IR1: Slightly irregular.
                - IR2: Moderately Irregular.
                - IR3: Irregular.
                - **LotConfig:** Lot configuration.
                - Inside: Inside lot.
                - Corner: Corner lot.
                - CulDSac: Cul-de-sac.
                - FR2: Frontage on 2 sides of property.
                - FR3: Frontage on 3 sides of property.
                """)
    
    st.divider()
    vars = [
        'LotFrontage',
        'LotArea',
        'OverallQual',
        'YearBuilt',
        'YearRemodAdd',
        'MasVnrArea',
        'BsmtFinSF1',
        'TotalBsmtSF',
        '1stFlrSF',
        '2ndFlrSF',
        'GrLivArea',
        'FullBath',
        'HalfBath',
        'TotRmsAbvGrd',
        'Fireplaces',
        'GarageYrBlt',
        'GarageCars',
        'GarageArea',
        'WoodDeckSF',
        'OpenPorchSF',
        'MSZoning_FV',
        'MSZoning_RH',
        'MSZoning_RL',
        'MSZoning_RM',
        'LotShape_IR2',
        'LotShape_IR3',
        'LotShape_Reg',
        'LotConfig_CulDSac',
        'LotConfig_FR2',
        'LotConfig_FR3',
        'LotConfig_Inside',
        'Neighborhood_Blueste',
        'Neighborhood_BrDale',
        'Neighborhood_BrkSide',
        'Neighborhood_ClearCr',
        'Neighborhood_CollgCr',
        'Neighborhood_Crawfor',
        'Neighborhood_Edwards',
        'Neighborhood_Gilbert',
        'Neighborhood_IDOTRR',
        'Neighborhood_MeadowV',
        'Neighborhood_Mitchel',
        'Neighborhood_NAmes',
        'Neighborhood_NPkVill',
        'Neighborhood_NWAmes',
        'Neighborhood_NoRidge',
        'Neighborhood_NridgHt',
        'Neighborhood_OldTown',
        'Neighborhood_SWISU',
        'Neighborhood_Sawyer',
        'Neighborhood_SawyerW',
        'Neighborhood_Somerst',
        'Neighborhood_StoneBr',
        'Neighborhood_Timber',
        'Neighborhood_Veenker',
        'HouseStyle_1.5Unf',
        'HouseStyle_1Story',
        'HouseStyle_2.5Unf',
        'HouseStyle_2Story',
        'HouseStyle_SFoyer',
        'HouseStyle_SLvl',
        'RoofStyle_Gable',
        'RoofStyle_Gambrel',
        'RoofStyle_Hip',
        'RoofStyle_Mansard',
        'RoofStyle_Shed',
        'Exterior1st_AsphShn',
        'Exterior1st_BrkComm',
        'Exterior1st_BrkFace',
        'Exterior1st_CBlock',
        'Exterior1st_CemntBd',
        'Exterior1st_HdBoard',
        'Exterior1st_MetalSd',
        'Exterior1st_Plywood',
        'Exterior1st_Stucco',
        'Exterior1st_VinylSd',
        'Exterior1st_Wd Sdng',
        'Exterior1st_WdShing',
        'Exterior2nd_AsphShn',
        'Exterior2nd_Brk Cmn',
        'Exterior2nd_BrkFace',
        'Exterior2nd_CBlock',
        'Exterior2nd_CmentBd',
        'Exterior2nd_HdBoard',
        'Exterior2nd_ImStucc',
        'Exterior2nd_MetalSd',
        'Exterior2nd_Plywood',
        'Exterior2nd_Stone',
        'Exterior2nd_Stucco',
        'Exterior2nd_VinylSd',
        'Exterior2nd_Wd Sdng',
        'Exterior2nd_Wd Shng',
        'ExterQual_Fa',
        'ExterQual_Gd',
        'ExterQual_TA',
        'Foundation_CBlock',
        'Foundation_PConc',
        'Foundation_Slab',
        'Foundation_Stone',
        'Foundation_Wood',
        'BsmtQual_Fa',
        'BsmtQual_Gd',
        'BsmtQual_TA',
        'BsmtExposure_Gd',
        'BsmtExposure_Mn',
        'BsmtExposure_No',
        'BsmtFinType1_BLQ',
        'BsmtFinType1_GLQ',
        'BsmtFinType1_LwQ',
        'BsmtFinType1_Rec',
        'BsmtFinType1_Unf',
        'HeatingQC_Fa',
        'HeatingQC_Gd',
        'HeatingQC_Po',
        'HeatingQC_TA',
        'KitchenQual_Fa',
        'KitchenQual_Gd',
        'KitchenQual_TA',
        'GarageType_Attchd',
        'GarageType_Basment',
        'GarageType_BuiltIn',
        'GarageType_CarPort',
        'GarageType_Detchd',
        'GarageFinish_RFn',
        'GarageFinish_Unf']

    x = [76.0, 7630, 5, 1900, 1996, 0.0, 0, 360, 1032, 780, 1812, 2, 0, 8,
        1, 1999.0, 2, 672, 344, 0, False, False, False, True, False,
        False, True, False, False, False, True, False, False, False,
        False, False, False, False, False, False, False, False, False,
        False, False, False, False, True, False, False, False, False,
        False, False, False, False, False, False, True, False, False,
        True, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, True, False, False,
        False, False, False, False, False, False, False, False, False,
        False, False, True, False, False, False, True, False, False,
        False, False, False, False, True, False, False, False, True,
        False, False, False, False, True, False, True, False, False,
        False, True, False, False, False, False, False, True, False,
        True]

    for i in range(len(vars)):
        if i < 20:
            x[i] = st.number_input(vars[i], value=x[i])
        elif i < 30:
            x[i] = st.selectbox(vars[i], [1,0], index=0 if x[i] else 1)
        else:
            x[i] = 1 if x[i] else 0

    st.divider()
    st.header('Results')
    result = st.button('Show the Expected Value of the House Price')
    if result:
        y = (-197898.5074964817) + (-49.0642723718595)*x[0] + (0.20995410993111108)*x[1] + (9733.74734843757)*x[2] + (7.018512890424944)*x[3] + (193.89046128535196)*x[4] + (-5.70256764263727)*x[5] + (-1.5860246957501118)*x[6] + (3.98108216285668)*x[7] + (6.115402111209903)*x[8] + (25.275947208042112)*x[9] + (15.525906210204312)*x[10] + (3276.7056793747242)*x[11] + (1860.8416611667299)*x[12] + (1668.8471992394457)*x[13] + (6995.6222249358725)*x[14] + (-110.33610466939899)*x[15] + (5586.527660721738)*x[16] + (27.507762660607114)*x[17] + (21.971002218377635)*x[18] + (17.302979782511102)*x[19] + (31291.920193640413)*x[20] + (33258.48507919521)*x[21] + (37229.6738411142)*x[22] + (22642.976241564636)*x[23] + (3185.842289038227)*x[24] + (-63415.47606338537)*x[25] + (-1772.7298093539612)*x[26] + (6104.309584021568)*x[27] + (-10826.91643381635)*x[28] + (-11727.92134965061)*x[29] + (-969.9477572817523)*x[30] + (-2403.7764779948066)*x[31] + (8551.415628176555)*x[32] + (21209.32277162784)*x[33] + (19426.56019536109)*x[34] + (16397.29647705429)*x[35] + (34070.5503337656)*x[36] + (396.90685278029787)*x[37] + (12763.975761260976)*x[38] + (16187.957170787646)*x[39] + (-15720.842155145167)*x[40] + (4075.0840330821366)*x[41] + (5661.544455406523)*x[42] + (-16585.59549265351)*x[43] + (8674.344617537101)*x[44] + (47791.44977116714)*x[45] + (38670.91774197122)*x[46] + (7256.845131109475)*x[47] + (4005.2742431769784)*x[48] + (6515.853794143586)*x[49] + (16761.880804447057)*x[50] + (32261.264326765137)*x[51] + (28278.81049773271)*x[52] + (19396.967370496754)*x[53] + (28059.711670923316)*x[54] + (9250.832367905381)*x[55] + (10931.666462824465)*x[56] + (-3634.4368588869347)*x[57] + (-6994.806870209579)*x[58] + (2454.6111354968193)*x[59] + (1618.254754671023)*x[60] + (-5549.667641216199)*x[61] + (3040.828870532234)*x[62] + (-2108.765935321806)*x[63] + (11917.767880938516)*x[64] + (-7435.838784511849)*x[65] + (2596.6971220142623)*x[66] + (-34444.87376214395)*x[67] + (18415.57570368216)*x[68] + (3.637978807091713e-11)*x[69] + (5282.29477158992)*x[70] + (-5976.967902962524)*x[71] + (-2217.8256282994316)*x[72] + (1663.9266421193256)*x[73] + (-2874.0691439456905)*x[74] + (-1158.6002963444753)*x[75] + (-3782.5875878999404)*x[76] + (593.4280994109226)*x[77] + (-14017.519940128961)*x[78] + (7685.755621364469)*x[79] + (-6876.202802410946)*x[80] + (3.637978807091713e-12)*x[81] + (5282.294771589046)*x[82] + (878.7891118444004)*x[83] + (-11715.654908227742)*x[84] + (74.28300714519901)*x[85] + (-2461.3008333493203)*x[86] + (-18290.308539138357)*x[87] + (-10207.706994831344)*x[88] + (1985.2785880758474)*x[89] + (1409.82797741376)*x[90] + (-15068.154918660786)*x[91] + (16651.13515855996)*x[92] + (25509.503597198687)*x[93] + (22455.72324116373)*x[94] + (5089.043758643926)*x[95] + (2756.563814281433)*x[96] + (-631.2300468971263)*x[97] + (13593.319197753843)*x[98] + (-4251.0928689942775)*x[99] + (-25132.542593044695)*x[100] + (-19169.859462998305)*x[101] + (-21847.970396033154)*x[102] + (13297.596511212503)*x[103] + (-4244.995989509837)*x[104] + (-5045.502521076813)*x[105] + (-3376.2992145226053)*x[106] + (1006.5602788460274)*x[107] + (-10302.592157007486)*x[108] + (-3780.884662324702)*x[109] + (-12565.030668569823)*x[110] + (-5828.749486010273)*x[111] + (-2973.9776195246122)*x[112] + (0.0)*x[113] + (-3440.3384499450767)*x[114] + (-18319.283354206575)*x[115] + (-14465.255983188727)*x[116] + (-19720.033517356824)*x[117] + (23726.83987206853)*x[118] + (15861.403669456846)*x[119] + (22990.91576926696)*x[120] + (4661.804055676203)*x[121] + (18016.888275657606)*x[122] + (-2264.942966808627)*x[123] + (-2492.7171706337795)*x[124]
        st.write(y)

##################################################################################################################################

elif option == '* Full Notebook with Output':
    html = """<iframe src="https://www.kaggle.com/embed/youssefspasha/predict-house-prices-linear-regression-eda?kernelSessionId=96854216" height="800" style="margin: 0 auto; width: 100%; max-width: 950px;" frameborder="0" scrolling="auto" title="Predict house prices-linear Regression-EDA "></iframe>"""
    st.markdown(html, unsafe_allow_html=True)



elif option == 'Basic Libraries & Data':
    st.header('Basic Libraries & Data')
    st.subheader('Import Basic Libraries')
    st.code("""
            import numpy as np
            import pandas as pd

            import matplotlib.pyplot as plt
            import seaborn as sns""",language='python')
    st.subheader('Import Data & Overview')
    st.code('''
            df_train = pd.read_csv('train.csv')
            df_test = pd.read_csv('test.csv')''', language='python')
    st.code('''df_train.head()''',language='python')
    df_train
    st.code('''df_train.info()''',language='python')
    st.code('''df_train.describe().round(2)''',language='python')
    st.write(df_train.describe())
    st.markdown('''
                conclusion:
                
                - There are missing values.
                
                - There are columns we may not need.''')

elif option == 'EDA':
    st.header('Exploratory Data Analysis')
    st.code('''
            # Check if all variables in both data (train, test) are identical except for the response variable ('SalePrice')
            (df_train.columns.drop('SalePrice') == df_test.columns).any()''', language='python')
    st.code('''
            # drop 'Id' columns from data train dataframe
            df_train.drop(["Id"], axis=1, inplace=True)
            # drop 'Id' column from test dataframe and save it in (id_test_list) to use it in submission.
            id_test_list = df_test["Id"].tolist()
            df_test.drop(["Id"], axis=1, inplace=True)''', language='python')
    st.code('''
            # Define Numeric and Categorical columns.
            numerical_cols = []
            categorical_cols = []

            for col in df_train.columns:
                if df_train[col].dtype in('int64','float64'):
                    numerical_cols.append(df_train[col].name)
                else:
                    categorical_cols.append(df_train[col].name)''', language='python')
    st.code('''
            # save numerical and categorigal data in independent dataframes for train and test data.

            numerical_df_train = df_train[numerical_cols]
            categorical_df_train = df_train[categorical_cols]

            numerical_df_test = df_test[numerical_cols[0:-1]]
            categorical_df_test = df_test[categorical_cols]''', language='python')
    st.code('numerical_df_train.columns', language='python')
    st.code('categorical_df_test.columns', language='python')
    st.header('EDA with numerical data')
    st.subheader('1. Investigate the distributions.')
    st.code("numerical_df_train.hist(figsize=(15,20), bins=30, color='blue', edgecolor='black');", language='python')
    st.code("""
            # We notice that there are some columns that center most of their values around a single value
            # drop columns with low variance (since they don’t meaningfully contribute to the model’s predictive capability)

            from sklearn.feature_selection import VarianceThreshold

            thresholder = VarianceThreshold(threshold=0.15)   # column where 85% of the values are constant
            data_high_variance = thresholder.fit(numerical_df_train)""", language='python')
    st.code("""
            # drop column where 85% of the values are constant

            high_variance_list = []
            for col in numerical_df_train.columns:
                if col not in numerical_df_train.columns[thresholder.get_support()]:
                    high_variance_list.append(col)

            high_variance_list""", language='python')
    st.code("""
            df_train.drop(high_variance_list, axis=1, inplace=True)
            df_test.drop(high_variance_list, axis=1, inplace=True)""", language='python')
    st.subheader('2. Investigate the correlations.')
    st.code("""
            # plot correlation heatmap
            plt.figure(figsize = (20,15))

            corr_matrix = numerical_df_train.corr()
            mask =  np.triu(np.ones_like(corr_matrix, dtype=bool))
            corr_matrix[(corr_matrix < 0.3) & (corr_matrix > -0.3)] = 0
            sns.heatmap(corr_matrix, annot=True, mask=mask, linewidths=0.5,cmap='Blues', vmin=0, vmax=1)""", language='python')
    st.code("""
            # variables that have a low correlation with 'SalePrice' [less than 0.25 or -0.25]
            condition  = numerical_df_train.corr()['SalePrice'] < 0.25
            condition2 = numerical_df_train.corr()['SalePrice'] > -0.25
            low_corr_cols = (numerical_df_train.corr()[condition & condition2]['SalePrice'].index).to_list()
            low_corr_cols""", language='python')
    
    st.code("""
            # variables that have a high correlation with 'SalePrice'.
            high_corr_cols = [elem for elem in (numerical_df_train.columns).to_list() if elem not in low_corr_cols]
            high_corr_cols""", language='python')
    st.code("""
            # drop variables that have a low correlation with 'SalePrice'.
            for i in range(len(low_corr_cols)):
                if i in df_train.columns:
                    df_train.drop(low_corr_cols, axis=1, inplace=True)
                    df_test.drop(low_corr_cols, axis=1, inplace=True)""", language='python')
    st.code("""
            # plot the correlation of each feature with SalePrice (only high correlation feature)
            for col in high_corr_cols:
                sns.jointplot(x=numerical_df_train.loc[:,col],y=numerical_df_train.loc[:,'SalePrice'], kind='reg', color='blue');""", language='python')
    st.write("""
             conclusion:
             - There are outlires that must be dealt with, as they may lead to misleading conclusions (This is shown in the scatter charts).</span>
             - There are some feather that have a strong correlation with the response variable (SalePrice) and others have a weak correlation.""")
    st.header('EDA with categorical data')
    st.code("""
            # create categorical dataframe and add 'SalePrice' column.
            categorical_cols.append('SalePrice')
            categorical_df_train = df_train[categorical_cols]
            categorical_df_train.columns""", language='python')
    st.subheader('1. Investigate the distributions.')
    st.code("""
            fig, axes = plt.subplots(15, 3, figsize=(18, 50))
            i = 0
            j = 0
            for col in categorical_df_train.columns:
                if j==3:
                    i += 1
                    j = 0
                    sns.countplot(x=categorical_df_train[col], data=categorical_df_train, ax=axes[i,j])
                else:
                    sns.countplot(x=categorical_df_train[col], data=categorical_df_train, ax=axes[i,j])
                j += 1""", language='python')
    st.code("""
            # variables are highly dominated by one feature (more than 90%).

            high_dominated_features = []
            for col in categorical_df_train.columns:
                if (categorical_df_train[col].value_counts().max()/categorical_df_train[col].count()) > 0.9:
                    high_dominated_features.append(col)
                    
            high_dominated_features""", language='python')
    st.code("""
            df_train.drop(high_dominated_features, axis=1, inplace=True)
            df_test.drop(high_dominated_features, axis=1, inplace=True)""", language='python')
    
    st.subheader("2. Describe 'SalePrice' with each categorical feature.")
    st.code("""
            fig, axes = plt.subplots(15, 3, figsize=(18, 50))
            i = 0
            j = 0
            for col in categorical_df_train.columns:
                if j==3:
                    i += 1
                    j = 0
                    sns.boxplot(x=col, y="SalePrice", data=categorical_df_train, ax=axes[i,j])
                else:
                    sns.boxplot(x=col, y="SalePrice", data=categorical_df_train, ax=axes[i,j])
                j += 1""", language='python')
    st.code('df_train.columns', language='python')
    st.code('df_test.columns', language='python')


elif option == 'Data Preprocessing':
    st.header('Data Preprocessing')
    st.header('Data Cleaning')
    st.subheader('1. deleting duplicate values')
    st.code("""
            print('number of duplicate values in numerical_df_train dataframe: ',numerical_df_train.duplicated().sum())
            print('number of duplicate values in numerical_df_test dataframe: ',numerical_df_test.duplicated().sum())
            print('number of duplicate values in categorical_df_train dataframe: ',categorical_df_train.duplicated().sum())
            print('number of duplicate values in numerical_df_test dataframe: ',categorical_df_test.duplicated().sum())
            """, language='python')
    st.code("""
            df_train.drop_duplicates(inplace=True)
            df_test.drop_duplicates(inplace=True)
            """, language='python')
    st.code("""
            # confirm changes
            print('number of duplicate values in df_train dataframe: ',df_train.duplicated().sum())
            print('number of duplicate values in df_test dataframe: ',df_test.duplicated().sum())
            """, language='python')
    
    st.subheader('2. Missing values')
    st.code("""
            fig, axes = plt.subplots(2, 2, figsize=(15,20))
            sns.heatmap(numerical_df_train.isnull(), ax=axes[0,0])
            sns.heatmap(numerical_df_test.isnull(), ax=axes[0,1])
            sns.heatmap(categorical_df_train.isnull(), ax=axes[1,0])
            sns.heatmap(categorical_df_test.isnull(), ax=axes[1,1])
            """, language='python')
    st.code("""
            # drop columns with missing more than 30%
            def drop_missing(df):
                i = 0
                for col in df:
                    if (df[col].isnull().sum()/1460) > 0.3:
                        df.drop(col, axis=1, inplace=True)
                        print('column',col,'is dropped')
                        i += 1
                if i == 0:
                    print('no column dropped')
            """, language='python')
    st.code('drop_missing(df_train)', language='python')
    st.code('drop_missing(df_test)', language='python')
    st.code("""
            def fill_null(df):
            for col in df:
                if (col in numerical_cols) & (df[col].isnull().any()):
                    df[col].fillna(df[col].mean(), inplace = True)
                    print('fillna numerical column: ',col)
                if (col in categorical_cols) & (df[col].isnull().any()):
                    df[col].fillna(df[col].mode().iloc[0], inplace = True)
                    print('fillna categorical column: ',col)""", language='python')
    st.code('fill_null(df_train)', language='python')
    st.code('fill_null(df_test)', language='python')
    st.code("""
            # confirm changes
            fig, axes = plt.subplots(1, 2, figsize=(15,15))
            sns.heatmap(df_train.isnull(), ax=axes[0])
            sns.heatmap(df_test.isnull(), ax=axes[1])
            """, language='python')
    
    st.subheader('3. Detect and remove outlires')
    st.code("""
            fig, axes = plt.subplots(13, 3, figsize=(18, 50))
            i = 0
            j = 0
            for col in numerical_df_train.columns:
                if j==3:
                    i += 1
                    j = 0
                    sns.boxplot(x=numerical_df_train[col],data=numerical_df_train, palette="Set2", ax=axes[i,j])
                else:
                    sns.boxplot(x=numerical_df_train[col],data=numerical_df_train, palette="Set2", ax=axes[i,j])
                j += 1
            """, language='python')
    st.code("""
            Q1 = np.percentile(df_train['SalePrice'], 25, interpolation = 'midpoint')
            Q3 = np.percentile(df_train['SalePrice'], 75, interpolation = 'midpoint')
            IQR = Q3 - Q1
            # Upper bound
            upper = np.where(df_train['SalePrice'] >= (Q3+1.5*IQR))
            # lower bound
            lower = np.where(df_train['SalePrice'] <= (Q1-1.5*IQR))
            # drop outlires
            df_train.drop(upper[0], errors='ignore', inplace = True)
            df_train.drop(lower[0], errors='ignore', inplace = True)
            """, language='python')
    st.code("""
            fig, axes = plt.subplots(13, 3, figsize=(18, 50))
            i = 0
            j = 0
            for col in df_train:
                if col in numerical_cols:
                    if j==3:
                        i += 1
                        j = 0
                        sns.boxplot(x=df_train[col],data=df_train, palette="Set2", ax=axes[i,j])
                    else:
                        sns.boxplot(x=df_train[col],data=df_train, palette="Set2", ax=axes[i,j])
                    j += 1
            """, language='python')
    
    st.subheader('Converting categorical values to numerical')
    st.code("""
            numerical_cols_new = []
            categorical_cols_new = []

            for col in df_train.columns:
                if df_train[col].dtype in('int64','float64'):
                    numerical_cols_new.append(df_train[col].name)
                else:
                    categorical_cols_new.append(df_train[col].name)
            """, language='python')
    st.code("""
            train_dummies = pd.get_dummies(df_train[categorical_cols_new], drop_first=True)
            test_dummies = pd.get_dummies(df_test[categorical_cols_new], drop_first=True)
            """, language='python')
    st.code("""
            df_train.drop(categorical_cols_new, axis=1, inplace=True)
            df_test.drop(categorical_cols_new, axis=1, inplace=True)
            """, language='python')
    st.code("""
            df_train = df_train.join(train_dummies)
            df_test = df_test.join(test_dummies)
            """, language='python0')
    st.code("""
            for col in df_train:
                if (col not in df_test.columns) & (col != 'SalePrice'):
                    df_train.drop(col, axis=1, inplace=True)
                    
            for col in df_test:
                if col not in df_train.columns:
                    df_test.drop(col, axis=1, inplace=True)
            """, language='python')
    st.code('df_train.info()', language='python')
    st.code('df_test.info()', language='python')

elif option == 'Preparing Data & Modeling':
    st.header('Preparing Data & Modeling')
    st.header('Build Linear Regression Model')
    st.subheader('Splitting the data into training and test sets ')
    st.code('from sklearn.model_selection import train_test_split', language='python')
    st.code("""
            # define response and explanatory variable.

        # response variable
        y = df_train['SalePrice']
        # explanatory variable
        X = df_train.drop('SalePrice', axis=1)
            """,  language='python')
    st.code('X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)', language='python')

    st.subheader('build model and fit data')
    st.code('from sklearn.linear_model import LinearRegression', language='python')
    st.code("""
            lmodel = LinearRegression()
lmodel.fit(X_train,y_train)
            """, language='python')
    
    st.subheader('model prediction')
    st.code("""
            y_pred = lmodel.predict(X_test)
y_pred
            """, language='python')
    st.code('y_test.values', language='python')

    st.subheader(' model evaluation')
    st.code('from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score', language='python')
    st.code('mean_absolute_error(y_test,y_pred)', language='python')
    st.code("""
            # RMSE
np.sqrt(mean_squared_error(y_test,y_pred))
            """, language='python')
    st.code("""
            # R^2
r2_score(y_test,y_pred)""", language='python')
    
    st.subheader('predict test dataframe and submission')
    st.code("""
            subm_preds = lmodel.predict(df_test)
subm_preds
            """, language='python')
